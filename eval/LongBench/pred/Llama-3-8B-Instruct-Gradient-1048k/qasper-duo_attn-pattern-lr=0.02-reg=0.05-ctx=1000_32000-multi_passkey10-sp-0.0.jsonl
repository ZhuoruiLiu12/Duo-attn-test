{"pred": "The ground truth for fake news is established through manual inspection of the text field within the tweets to label them as containing fake news, or not containing them.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and excludes them during feature aggregation. It is used for language identification and has been shown to outperform other pooling methods by achieving 98.43% F1-Score.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Their model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes using context tweets as an additional feature for neural network models to better understand the data and detect abusive language. The article also suggests using ensemble models of variant models and features for further improvements.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at different Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. They also used a subset of pages based on their performance on the development set and the observation of emotions distribution on different pages and in the different datasets.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes. The article states that the hashtag segmentation model is language-independent and the authors intend to extend their toolkit to languages other than English as future work.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The article proposes an evaluation protocol and baseline for the task of concept-map-based MDS. The corpus is also made publicly available under a permissive license.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The article evaluated the proposed approach on three single-document news summarization datasets representative of different writing conventions and summary styles. The datasets used for evaluation are the CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach outperforms existing approaches on benchmark word similarity and entailment datasets.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by selecting the best performing model that had not been previously tried and adding it to the ensemble if it improves validation performance, and discarding it otherwise. The algorithm tries each model once and forms a greedy ensemble. The final ensemble is formed by simply averaging the predictions from the constituent single models.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The Friends dataset comes from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. Each subset includes 1,000 English dialogues, and each dialogue can be further divided into a few consecutive utterances. All the utterances are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. Annotator votes for one of the seven emotions, namely Ekman’s six basic emotions plus the neutral. If none of the emotion gets more than three votes, the utterance will be marked as “non-neutral", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves significantly better performances than previous studies on crowdsourcing learning, outperforming all other models (the p-value is below $10^{-5}$ by using t-test).", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they conducted a detailed technical validation of the data as proof of the quality of the recordings. They also compared the results to ZuCo 1.0, which allows for a more direct comparison due to the analogous recording procedure. Additionally, they analyzed the differences in the human processing of normal reading versus annotation. They also extracted various engineered eye-tracking features and sentence-level EEG features, which can be used to improve and evaluate NLP and machine learning methods. Finally, they provided the first dataset of simultaneous eye movement and brain activity recordings to analyze and compare normal reading to task-specific reading during annotation. The corpus is tailored to cognitively", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The article mentions using a set of thousand documents related to finance to create domain-specific word vectors, and a set of 246,945 documents corresponding to 184,001 Twitter posts and 62,949 news articles, all related to finance, to create domain-specific word vectors. Additionally, the article mentions using a public available corpus for conversational systems and a set of tasks for the Dialog State Tracking Challenge (DSTC) for collecting annotated task-specific dialogs.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The Energy sector achieved the best performance with an accuracy of 0.44. However, the proposed Global model approach demonstrated outperformance across all sectors, indicating that the characteristics of the results reported in the article permeated all sectors, rather than being composed of a mix of outperforming and underperforming sector contributions. Therefore, the outperformance of the proposed model with respect to GARCH(1,1) was not the result of a learning bias to a given sector or stock.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of the SMT and various NMT models on their built dataset, including the RNN-based NMT model and the Transformer model. They also tested the basic NMT model with several techniques, such as target language reversal, residual connection, and word2vec. For word embedding pre-training, they collected an external ancient corpus which contains 134M tokens.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, UTCNN without user information, UTCNN without topic information, UTCNN without comments, and UTCNN without comments.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved the state-of-the-art performance.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Their model improves interpretability by allowing for sparse attention weights, which can lead to crisper examples of attention head behavior and novel behaviors unraveled thanks to the sparsity and adaptivity of their proposed model. Additionally, their model can dynamically select a sparsity pattern that finds relevant words regardless of their position, which may provide new ideas for designing static variations of the Transformer. Overall, their model shows potential in both translation accuracy as well as in model interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline model used for back-translation and the DocRepair model are both Transformer base models. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article uses XNLI test accuracy and Labeled Attachment Scores (LAS) for evaluation.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on ASR, MT, and ST tasks respectively.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The article mentions that most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc).", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines were not mentioned in the article.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "They look at over 20,000 blog users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, and human evaluation.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for each symptom/attribute, which can take on different linguistic expressions defined as entities. If the queried symptom or attribute is not mentioned in the dialogue, the groundtruth output is “No Answer”.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The article does not mention a specific amount of data needed to train the task-specific encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The article uses four machine translation tasks for evaluation: IWSLT 2017 German $\\rightarrow$ English, KFTT Japanese $\\rightarrow$ English, WMT 2016 Romanian $\\rightarrow$ English, and WMT 2014 English $\\rightarrow$ German.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The results show that the newly produced contextual embeddings produce substantially better results compared to the non-contextual fastText baseline. In future work, we plan to use the produced contextual embeddings on the problems of news media industry. The pretrained ELMo models will be deposited to the CLARIN repository by the time of the final version of this paper.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "The authors have diverse disciplinary backgrounds and research practices.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Unanswerable. The paper proposes a supervised approach to spam detection using topic-based features extracted from LDA model. The effectiveness of the proposed features is evaluated using a public dataset and a self-collected dataset. The paper also compares the proposed features with previously used features for spammer detection. The authors conclude that the proposed features outperform all other features in terms of F1-score and recall, and achieve a great advantage over UFN+UC+UH in terms of recall and F1-score. The authors also suggest future work, such as improving the combination method of local and global information, applying decision theory to enhance the performance of", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared 6-layers and 9-layers sMBR models.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. They constructed the dataset by first crawling all articles from each quality class repository, e.g., they get FA articles by crawling pages from the FA repository: https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on an under-resourced language pair and a zero-resourced translation task. They also propose a variant of their strategy that incorporates French-French data. They compare their proposed systems to a direct system, which is an NMT trained on German-English and English-French parallel corpora, and to a pivot system, which consists of two separate NMTs trained to translate from German to English and English to French. They report the performance of those systems in Table TABREF23. However, they note that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated based on the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, and the accuracy of a scheme, which is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The efficiency of a communication scheme is also quantified by the retention rate of tokens, and the accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The effectiveness of constrained objective is also evaluated by comparing the achievable accuracy and efficiency tradeoffs for the two objectives. The robustness of learned communication", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are looked at for classification tasks.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the existing domain with labeled data, while the target domain is a new domain with very few or no labeled data.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art methods, including RAN, QRNN, and NAS.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes embedding layers, neural network layers such as RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, and regularization layers such as Dropout, Layer Norm, Batch Norm. It also offers more options for loss functions and metrics depending on the task.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and the Carnegie Mellon Pronouncing Dictionary for g2p. They also used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. Additionally, they used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10. In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were BERT, RoBERTa, and XLNet.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "Spanish and Finnish.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use Glove embeddings for initializing the embeddings of the top 20K words in the corpus.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Unanswerable. The article does not mention any baseline evaluation for PolyReponse.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use LIWC to create maps for word categories that reflect certain psycholinguistic or semantic properties. They also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components such as claims, premises, backing, rebuttal, and refutation.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "N-grams of order n are aligned using PARENT.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Hebrew, Estonian, Finnish, Spanish, Polish, Arabic, and Farsi.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The model is applied to two datasets: the `Conversations Gone Awry' dataset and the subreddit ChangeMyView (CMV) dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Unanswerable. The article does not mention deep learning models. It only mentions using Freeling library and training a different Portuguese dependency parsing model that was compatible with the available annotated dataset.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated through various sanity checks, including sentence-level BLEU scores, manual inspection of identical source transcripts and translations, perplexity of translations, ratio of English characters in translations, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They combine audio and text sequences using a feed-forward neural model.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI. It verified that including synthetic data during training is very effective, and yields an improvement over our baseline NMF by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The article does not mention how many humans evaluated the results. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "A tweet goes viral if it is retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "BERT", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The DeepMine database was collected using crowdsourcing.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Machine learning and deep learning methods used for RQE include Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs).", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the social honeypot dataset, and its quality is extensively explored in the paper. However, the quality of the dataset is not explicitly stated. Therefore, the answer is \"unanswerable\".", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM decoder that generates the characters in the output word form using encoder states and an attention mechanism.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Unanswerable. The article does not mention the language of the data used in their experiments.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (II and IV) from each of the folds 1-3, which achieved a score of 0.673 in F1 on dev (external) for the SLC task and 0.673 in F1 on dev (external) for the FLC task.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a weak NMT model without using any monolingual data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "Their highest recall score was 0.7033 in test batch 4.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores word embedding techniques such as word2vec. However, it also discusses the limitations of purely distributional methods and proposes a solution to improve their performance by integrating semantic similarity and relatedness values. The paper also compares its results to other studies that have applied various word embedding methods to the same reference standards used in the study. Overall, the paper suggests that integrating semantic similarity and relatedness values has the potential to improve the performance of purely distributional methods.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use a bilingual dictionary to translate each word in the source language into English. In an end-to-end solution, it would have been ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings, but the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No. The paper mentions that BioIE systems aim to extract information from a wide spectrum of articles including medical literature, biological literature, electronic health records, etc. However, it does not specifically explore extraction from electronic health records.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts used for annotation were seven individuals with legal training. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The article uses a CNN-RNN based image-to-poem net for painting embedding and a sequence-to-sequence model with parallel text corpus for language style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The article proposes two extensions to the BERT model, Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT), which enable its application in classification of long texts by performing segmentation and using another layer on top of the segment representations. The article also presents state-of-the-art results on the Fisher topic classification task and significant improvement on the CSAT prediction task over the MS-CNN model. The article does not directly compare the performance of the RNN layer and the transformer layer on top of BERT.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes. The authors believe that humans' robustness to noise is due to their ability to utilize general knowledge beyond what is contained in a given passage-question pair. They propose a data enrichment method that extracts inter-word semantic connections from each passage-question pair and uses this general knowledge to assist the attention mechanisms of their MRC model, which they call Knowledge Aided Reader (KAR). They also suggest that integrating neural networks with general knowledge can bridge the gap between MRC models and human beings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. They also use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, they force the network to pay special attention to it. The two contexts are processed by two independent convolution", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The dataset contains three major classes: Person (PER), Location (LOC), and Organization (ORG).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is higher quality when expert annotations are used, with a bigger difference between the expert and crowd models on the difficult set. However, the difference in performance when expert annotations are used is less than 1% when compared to the model trained with crowd labels with respect to F1. Additionally, the model trained with re-annotating the difficult subset (D+Other) outperforms the model with re-annotating the random subset (R+Other) by 2 points in F1. The model trained with re-annotating both of difficult and random subsets (D+R+Other), however, achieves", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The imbalance in analyzed corpora is significant, with 65% of speakers being men and speaking more than 75% of the time. Women represent 33.16% of the speakers, but only 22.57% of the total speech time. Women are also less present in important roles, with only 29.47% of speakers within the Anchor category being female. Their percentage of speech is also smaller, and when calculating the average speech time uttered by a female Anchor, it is 15.9 min against 25.2 min for a male Anchor. This confirms the existence of gender disparities within French media. It", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German dataset.", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to the proposed model in terms of closed test setting.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The article discusses the use of discriminative models, such as deep neural networks, for event detection on microblogging platforms. However, the article does not specify which type of classifiers are used. Therefore, the answer is \"unanswerable\".", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26, and BIBREF27.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. However, the usefulness of Flickr for characterizing the natural environment is less well-understood.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes. They use attention in the memory generation layer.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "The authors used three datasets for evaluation: CSAT, 20 newsgroups, and Fisher Phase 1 corpus.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, some of these tasks were evaluated in previous work. BIBREF1, BIBREF2, and BIBREF3 all evaluated different aspects of syntax-sensitive phenomena using uni-directional language-model-like models. However, the BERT model is bi-directional and was adapted to the bidirectional nature of the stimuli used in these previous works. The results are not directly comparable to previous work due to differences in training data, evaluation protocol, and stimuli used. Nonetheless, the high performance numbers indicate that the purely attention-based BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a constraint on the neural projector that requires it to be invertible and have a unit Jacobian determinant, which enables tractable exact inference and marginal likelihood computation. This condition is necessary to address the optimization challenge in the proposed approach.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema categorizes gold standards according to linguistic complexity, required reasoning, and factual correctness, and includes a metric based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task. It also includes categories for different types of expected answers, such as Span, Paraphrasing, Unanswerable, and Generated. The schema also includes categories for different types of reasoning, such as Temporal succession of events, Spatial reasoning, and Causal reasoning, as well as categories for linguistic phenomena, such as Redundancy, Synonyms, and Nominalisation. The schema also includes categories for distractors, which", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "WikiSmall dataset has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge dataset has 296,402 sentence pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines include the vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pre-train baseline, and cascaded systems.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "Unanswerable. The article does not mention any specific natural language(s).", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs, BiLSTM, and CNN models are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Unanswerable.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings were used.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Their personalized models outperformed the baseline in BPE perplexity, and generated more diverse and acceptable recipes than the baseline. The Prior Name model achieved the best user matching accuracy and Mean Reciprocal Rank by a large margin, revealing that prior recipe names are strong signals for personalization. The addition of attention mechanisms to capture these signals improved language modeling performance over a strong non-personalized baseline.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to state-of-the-art results on existing, standard evaluation datasets.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution results showed that viral tweets containing fake news were created more recently, had a larger proportion of friends/followers, and included more URLs than viral tweets without fake news. However, there were no apparent differences in retweets or favourites between the two sets of tweets. The content of viral fake news was also highly polarized, suggesting that textual sentiment analysis of the content of tweets, together with the above-mentioned parameters from meta-data, may prove useful for identifying fake news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The article does not mention any specific accents present in the corpus. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "A compact, scalable, and meaningful representation of a set of word vectors.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11. B2. The second baseline assigns the value relevant to a pair INLINEFORM0, if and only if INLINEFORM1 appears in the title of INLINEFORM2. S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2. S2: Place the news into the most frequent section in INLINEFORM0.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Unanswerable. The article does not provide information about whether SemCor3.0 is representative of English language data in general. It only states that it is the largest corpus manually annotated with WordNet sense for WSD.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The Augmented LibriSpeech dataset is 110 hours.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "They used the high-quality datasets released by the challenge organizers for ternary and fine-grained sentiment classification. The fine-grained dataset was split into training, development, development_test, and test parts. The ternary dataset was not mentioned as being split into different parts.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning, which has 12 Transformer blocks, 768 hidden layers, 12 self-attention heads, and 110M total parameters.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Unanswerable.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Unanswerable. The article does not mention anything about the domain of the images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets. They achieved an average f-score of 0.368 on the development set using a simple tf-idf bag-of-word model, and their best model (B-M) on development data relied entirely on automatically obtained information, both in terms of training data as well as features.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is a novel one consisting of three tags, namely {INLINEFORM0}, {INLINEFORM1}, and {INLINEFORM2}.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No, Arabic is not one of the 11 languages in CoVoST.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The article defines robustness of a model as the ability to handle situations where the prior knowledge supplied to the learning model may be biased or have heavy load of instance annotation. The proposed methods aim to make the model more robust and practical by reducing the sensibility of the prior knowledge and making it more robust.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "InferSent and Universal Sentence Encoder are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets including CoNLL2003 and OntoNotes5.0, and achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively for Chinese datasets.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against syntactic tree-based models, latent tree models, and non-tree models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the improved relation detection model proposed in the article.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the Encoder-Decoder baseline with ingredient attention (Enc-Dec) and the Nearest-Neighbor model (NN).", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The article discusses several methods to manually detect stereotypes, biases, and odd phrases, including tagging all descriptions with part-of-speech information and leveraging the structure of Flickr30K Entities. However, the article acknowledges that it is difficult to find an automated measure for detecting bias from the data due to the richness of language.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "They explore French, Spanish, Italian, Portuguese, Arabic, Hebrew, and English.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Unanswerable. The article does not mention the language(s) used in their experiments.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with a few summarization algorithms provided by the Sumy package. They also proposed an ILP-based summarization technique to produce a summary of peer feedback comments for a given employee and compared it with manual summaries.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was proposed by BIBREF0, which used probabilistic graphical models to model structure and sequence, but required a hyperparameter for the number of latent states.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The neighbors-only experiment showed that ignoring the previous feature of the node itself when updating its representation completely deteriorated performance across all datasets, stressing the need to take into account the root node during updates, not only its neighborhood.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: \"year [tab", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Reasonable performance can be obtained even when the languages for training and testing are different.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model, ALOHA, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article does not provide a specific percentage or numerical improvement gained from Adversarial Reward Augmented Maximum Likelihood (ARAML).", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that many errors are due to biases from data collection and rules of annotation, and that some tweets with specific language and geographic restriction are oversampled, resulting in high rates of misclassification. They also note that annotators tended to annotate many samples containing disrespectful words as hate or offensive without any presumption about the social context of tweeters, whereas they were just offensive or even neither tweets. However, they also observe that despite the bias in the data, their model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding that it has, and this may be the main reason for high misclassifications of hate samples as", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, three baselines were tested to compare with the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The size of the dataset is not specified in the article. However, it is mentioned that the OurNepali dataset is almost ten times bigger compared to the ILPRL dataset in terms of entities.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article does not mention any specific datasets used. However, it does mention that the authors use a neural network pretrained as a language model and fine-tune it to directly predict ERP components. They also use eye-tracking and self-paced reading data in their analysis.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of visual stimuli to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN are used as baselines for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The article investigates the efficacy of different learning models in detecting abusive language, including traditional machine learning classifiers and neural network models such as Convolutional Neural Networks and Recurrent Neural Networks.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "Bi-directional and uni-directional language models are used.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights of training examples are dynamically adjusted as training proceeds using a soft weighting scheme that emphasizes harder examples during training. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies using knowledge graphs and enhanced exploration methods far outperform the baseline A2C and KG-A2C, and both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual Bayesian model for each language.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified through annotations of the Spanish words interspersed in Mapudungun speech. Additionally, the corpus includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step. It treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. Each word, represented accordingly, is then fed into a BiLSTM cell. At each sequence step, the training target is the correct corresponding word (output dimension equal to vocabulary size), and the model is optimized with cross-entropy loss.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL consistently outperforms various baselines with a favorable generalization ability.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes. The article states that the recordings have an average duration of 9min 28s and have a verbatim transcript of 1,500 words on average (written by the experts). Both the audio and the transcript are de-identified (by removing the identifying information) with digital zeros and [de-identified] tags, respectively. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The summaries and Medication Regimen tags (MR tags) are also annotated with summaries and grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The transcript for", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the CE task in 2010 i2b2/VA.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the refine decoder to generate more fluent and natural sequences by concentrating on one word at a time, based on the source document as well as other words. This process is similar to the cloze task in BERT's pre-train process, and by using the ability of the contextual language model, the decoder can generate more fluent and natural sequences. Additionally, the refine decoding process provides a more complete input sequence which is consistent with BERT's pre-training processes, which helps to alleviate the issue of incomplete and inconsistent context that pre-trained context encoders like GPT or BERT may suffer", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article does not mention the specific dataset used. It only discusses various models for unsupervised tweet representation learning.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. The annotations are binarized as the positive class or negative class. The dataset contains 9,473 annotations for 9,300 tweets.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "They evaluated on eight publicly available NER tasks used in BIBREF2.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated into Spanish using the machine translation platform Apertium. Additionally, the lexicons in the AffectiveTweets package were translated to Spanish, and the English version of SentiStrength was replaced by the Spanish variant made available by BIBREF6. The optimal combination of lexicons was determined by adding beneficial lexicons until the score did not increase anymore.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the SLC task was a simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with baselines that do not adopt joint learning.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of the method, classification experiments were performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from 1.7K bilingual ancient-modern Chinese articles collected from the internet, with a large part of the ancient Chinese data coming from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\"unanswerable\" (the article does not mention any specific chinese datasets being used)", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has multiple layers, including convolutional layers, maximum pooling layers, average pooling layers, and a fully connected network. The exact number of layers is not specified in the article.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The European network of nature protected sites Natura 2000 dataset is used in this paper. Additionally, the ScenicOrNot website dataset is used for predicting people's subjective opinions of landscape beauty in Britain. The authors also use nine numerical features and 180 categorical features from scientific datasets. These include latitude, longitude, elevation, population, and five climate related features, as well as CORINE land cover classes at level 1, 2, and level 3, and 116 soil types (SoilGrids).", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of around 7,000 real medical reports written in Spanish and annotated with negation and uncertainty information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. The MEDDOCAN evaluation scenario could be said to be somewhat far from the real use case the technology developed for the shared task is supposed to be applied in. However, at the moment it also provides the only public means for a rigorous comparison between systems for sensitive health information detection in Spanish texts.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigrams and pragmatic features, stylistic patterns, patterns related to situational disparity, and hashtag interpretations.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are Coverage, Avg. MCC, and avg. +ve F1 score.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Unanswerable. The article does not mention anything about employing their indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The article describes experiments conducted on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. The experiments include automatic evaluations and human evaluation results. The article also presents some example outputs of different models and discusses issues encountered during the experiments, such as no change, word repetition, and improper words. Finally, the article describes additional experiments on the transformation from ironic sentences to non-ironic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. It ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian weight only relies on the distance between characters and ensures that the Gaussian weight equals 1 when the distance between characters is 0. The larger distance between characters is, the smaller the weight is, which makes one character can affect its adjacent characters more compared with other characters.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article does not specify which types of social media they considered. It only mentions Facebook status update messages as the source of their dataset. However, they mention that they used a Twitter-trained GLOVE word embedding for each token of extracted discourse arguments from messages. This suggests that they may have also considered Twitter data.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The network's baseline features are the features extracted from the fully-connected layer of the baseline CNN. They are used for the final classification of a sentence as sarcastic vs non-sarcastic. The baseline CNN extracts the inherent semantics from the sarcastic corpus by employing deep domain understanding. The process of using baseline features with other features extracted from the pre-trained model is described in Section SECREF24. The baseline features perform best among other features, and when combined with sentiment, emotion, and personality features, they achieve the highest performance. The baseline features alone are quite capable of detecting sarcasm. When we combine sentiment, emotion, and personality", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters was varied in the experiments on the four tasks. The k-means clusters were initialized using “k-means++” as proposed in BIBREF9, while the algorithm was run for 300 iterations. The experiments were repeated with different seed initialization for 10 times and the clustering result that minimized the cluster inertia was selected. The article also mentions that the same parametrization for the learning algorithm was kept for all experiments. However, it does not specify which hyperparameters were varied for the learning algorithm.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to previous methods such as customized rule-based pattern matching, a combination of rule-based and machine learning approaches, CNN variants, and LSTM variants. However, the model achieves strong performance across multiple datasets using a single learned model, which is a departure from previous work that failed to generalize to medical domain questions. The model also surpasses state-of-the-art performance on the GARD corpus of consumer health questions and MLBioMedLAT corpus of biomedical questions.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are significantly larger than the previous ones.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 6946 sentences.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "Eusboost and MWMOTE techniques.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes. Their NER model takes as input both image and text for recognition of a named entity in text input. To the best of their knowledge, their approach is the first work to incorporate visual contexts for named entity recognition tasks. They also propose a general modality attention module that selectively chooses modalities to extract primary context from, maximizing information gain and suppressing irrelevant contexts from each modality (they treat words, characters, and images as separate modalities). Their approach allows for re-use of the same model for predicting labels even when some of the modalities are missing in input, as other modalities would still preserve the same semantics in the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a corpus of English texts. However, they also mention that they use the one billion word language modeling benchmark dataset, which is not specific to English. Therefore, it is unclear if they evaluate on any non-English datasets.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "Their highest MRR score for Factoid Question Answering task was '0.6103' in one of the test batches.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The Wall Street Journal (WSJ) portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this claim by stating that \"building models under these frameworks requires a large overhead of mastering these framework details.\" This suggests that engineers often struggle with the complexity of these frameworks and need higher level abstraction to hide the framework details.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve the state of the art on both single-relation and multi-relation KBQA tasks.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
