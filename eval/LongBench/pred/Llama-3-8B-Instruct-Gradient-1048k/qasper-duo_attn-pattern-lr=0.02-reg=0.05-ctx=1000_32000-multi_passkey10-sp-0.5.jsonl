{"pred": "The ground truth for fake news is established through manual inspection of the text field of tweets by an expert. The annotation is carried out by a single person in order to obtain a consistent annotation throughout the dataset. The annotated dataset is publicly available and can be freely reused.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and excludes them during feature aggregation. It is used for language identification and has been shown to outperform other pooling methods.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Their model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes using context tweets as an additional feature for neural network models to improve the accuracy of detecting abusive language. The context tweets provide significant contextual information that can help computers better understand the meaning of a tweet. The article also suggests using ensemble models of variant models and features for further improvements.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at various Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. They also looked at pages related to specific events, such as Mother's Day in May 2016. They chose different pages based on intuition and availability, and then selected different combinations based on results of a basic model run on development data. Their final collection of Facebook pages for the experiments described in the paper is as follows: FoxNews, CNN, ESPN, New York Times,", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Unanswerable. The article does not specify whether the hashtag and SemEval datasets contain only English data or not. It only mentions that the hashtag segmentation model is designed for English hashtags.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The article proposes an evaluation protocol and baseline for the task of concept-map-based MDS. The corpus is also made publicly available under a permissive license.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The article evaluates the proposed approach on three single-document news summarization datasets representative of different writing conventions and summary styles. The datasets used are CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The article does not directly compare this approach to other WSD approaches employing word embeddings. However, it does compare the proposed approach to other approaches on the SCWS dataset and shows that it achieves better correlation than existing approaches for various metrics.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by selecting the best performing model that had not been previously tried and adding it to the ensemble if it improves validation performance, and discarding it otherwise. The algorithm tries each model once and forms a greedy ensemble. The final ensemble is formed by simply averaging the predictions from the constituent single models.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The sources of the datasets are the scripts of the Friends TV sitcom and Facebook messenger chats.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves the highest accuracy among all the systems tested, significantly outperforming the other models.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they conducted a technical validation of the data to ensure its quality and compared it to their previous dataset, ZuCo 1.0. They also compared the results to previous studies investigating co-registration of EEG and eye movement data during natural reading tasks. Additionally, they used the dataset to train and evaluate machine learning algorithms for NLP purposes. Therefore, the answer is \"yes\".", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The article mentions using a set of thousand documents related to finance, as well as 184,001 Twitter posts and 62,949 news articles, all related to finance, to create domain-specific word vectors. Additionally, the authors used a Wizard of Oz method to collect a set of 124 questions that users asked, which were manually classified into a set of intent classes and used to train the first version of the system. The training set was then increased both in terms of number of classes and samples per class, resulting in a training set with 37 classes of intents and a total of 415 samples, with samples per class ranging from", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The article does not mention which stock market sector achieved the best performance. It only mentions that the proposed Global model approach is able to generalize well and the outperformance of the model spreads across a diversified portfolio of sectors. Therefore, the answer is \"unanswerable\".", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the RNN-based NMT model, Transformer-NMT, and SMT models.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, UTCNN without user information, UTCNN without topic information, UTCNN without comments, and UTCNN without topic or comment information.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved by several points.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Their model improves interpretability by allowing for sparse solutions and piecewise-linear sparsemax attention, which can yield exactly zero probabilities for irrelevant words, and by enabling the sparsity and adaptivity of their proposed model to unravel novel behaviors in attention head behavior. Additionally, their model allows for different, adaptive α values for each attention head, which can lead to more variance in individual head behavior and overall to different tendencies compared to the fixed case of α =1.5. Overall, their model provides a better potential for analysis and can identify head specializations more easily due to the sparsity of their models.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline model used for back-translation and the DocRepair model are both Transformer base models.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article uses XNLI test accuracy and Labeled Attachment Scores (LAS) for evaluation. Additionally, the article evaluates the performance of RAMEN-based parser for supervised dependency parsing on UD datasets. The article also evaluates the performance of RAMEN+RoBERTa$_{\\textsc {base}}$ (initialized from monolingual data) at each training step. The results are presented in Figure FIGREF40. The article also evaluates the impact of initialization on the zero-shot performance of RAMEN$_{\\textsc {base}}$ by training a RAMEN$_{\\textsc {base}}$+RoBERTa with foreign word-", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on ASR, MT, and ST tasks respectively.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The article mentions that stylistic patterns and patterns related to situational disparity are obtained. However, it does not specify what these patterns are. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines were not mentioned in the article.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "They look at over 20,000 blog users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, and human evaluation.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels on their dataset based on the templates and expression pools they construct.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The article does not mention the amount of data needed to train the task-specific encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The article mentions four machine translation tasks for evaluation: IWSLT 2017 German $\\rightarrow$ English, KFTT Japanese $\\rightarrow$ English, WMT 2016 Romanian $\\rightarrow$ English, and WMT 2014 English $\\rightarrow$ German. All of these datasets were preprocessed with byte-pair encoding using joint segmentations of 32k merge operations. The models were trained on these datasets and evaluated on their test sets. The end-to-end computational overhead of the models using $\\alpha$-entmax and $1.5$-entmax is relatively small; in training tokens per second, the models", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The ELMo embeddings show significant improvements over fastText embeddings in the NER task, especially for Croatian and Lithuanian, and also for English and Finnish, which are among the largest datasets. Only on Slovenian dataset did ELMo perform slightly worse than fastText, on all other EMBEDDIA languages, the ELMo embeddings improve the results.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in the humanities and social sciences.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Unanswerable. The paper proposes a supervised approach to spam detection using topic-based features.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared 6-layers and 9-layers sMBR models with CE models, and also compared the 2-layers Shenma model with sMBR with the 2-layers regular-trained Amap model.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. They constructed the dataset by first crawling all articles from each quality class repository, e.g., they get FA articles by crawling pages from the FA repository: https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on under-resourced and zero-resourced translation tasks.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated based on the retention rate of tokens and the accuracy of the generated sentences. The efficiency of a communication scheme is measured as the fraction of tokens that are kept in the keywords, while the accuracy is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The system is also evaluated in a user study, where users are asked to type a set of keywords into the system and mark whether each of the top three suggestions from the autocomplete system is semantically equivalent to the target sentence. The system is also compared to two rule-based baselines: Unif and Stopword. The overall", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are looked at for classification tasks.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain with labeled data, while the target domain is the domain with very few or no labeled data.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art methods, including RAN, QRNN, and NAS.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes various neural network modules such as embedding layers, neural network layers like RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, and loss functions.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and the Carnegie Mellon Pronouncing Dictionary for g2p. They also used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. Additionally, they used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10. In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were BERT, RoBERTa, and XLNet.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "Spanish and Finnish.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on Named Entity Recognition, POS tagging, text classification, language modeling, and hashtag prediction for social media.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use 300-dimensional Glove embeddings for word embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Unanswerable. The article does not mention any baseline for PolyReponse's evaluation.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They obtain psychological dimensions of people through the analysis of psycholinguistic and semantic word categories.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components such as claims, premises, backing, rebuttal, and refutation.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "N-grams of order n are aligned using PARENT.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 1,873 conversation threads, roughly 14k tweets.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The article does not specify the 12 languages covered. It only mentions that the languages are typologically diverse and include under-resourced ones such as Welsh and Kiswahili. The article also mentions that the languages are from eight different language families and include languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. The article encourages researchers to create and submit Multi-SimLex-style datasets for additional languages.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The model is applied to two datasets, one of which is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9, and the other is constructed from conversations collected via the Reddit API. The prediction task is to forecast whether a conversation will be subject to moderator action in the future. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9, while the second dataset is constructed from conversations collected via the Reddit API. The prediction task is to forecast whether a conversation will be subject to moderator action in the future. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Unanswerable. The article does not mention deep learning models.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated through various sanity checks, including sentence-level BLEU, perplexity, ratio of English characters, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. The overlaps of train, development, and test sets are also checked to ensure they are disjoint. Additionally, the speaker demographics are inspected in terms of sample distributions with respect to speaker counts, accent counts, and age groups.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They combine audio and text sequences in their RNN by concatenating the final encoding vectors from the audio-RNN and text-RNN.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The article does not mention how many humans evaluated the results. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "A tweet going viral is defined as being retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "BERT", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The DeepMine database was collected using crowdsourcing.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Logistic Regression and deep learning models such as Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs) are used for RQE.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset, and its quality is not explicitly stated in the article. However, it is mentioned that the dataset has been extensively explored in the paper. Therefore, it can be assumed that the quality of the dataset is sufficient for the purpose of the research.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Unanswerable. The article does not mention the language of the data used in their experiments.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among author's submissions is the ensemble+ of (II and IV) on dev (external) set, which had a F1 score of 0.673. The author's submission is ranked 3rd in FLC task and 4th in SLC task. \n\nNote: The question is not a yes/no question, so the answer is not \"yes\" or \"no\".", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a weak NMT model that had never seen parallel data for the Ja INLINEFORM0 Ru pair. The performance on Ja INLINEFORM1 En pair was much lower than the two baseline models, reflecting the crucial mismatch between training and testing domains. In contrast, Ru INLINEFORM2 En pair benefited the most and achieved surprisingly high BLEU scores. The reason might be due to the proximity of out-of-domain training data and in-domain test data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "Their highest recall score was 0.7033 for List-type questions.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores word embedding techniques such as word2vec. However, the authors note that at their core, these approaches remain distributional techniques that typically start with a word by word co–occurrence matrix, much like many of the more traditional approaches. The authors also mention learning word, phrase, and sentence embeddings from structured corpora such as literature and dictionary entries. These embeddings could be integrated into a second–order vector or be used on their own.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use a bilingual dictionary to translate each word in the source language into English. In an end-to-end solution, it would have been ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings. However, the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations. They also found that these embeddings were not useful for transfer learning.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No, the paper does not mention extraction from electronic health records.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts used for annotation were seven individuals with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The article uses a CNN-RNN based image-to-poem net for painting embedding and a sequence-to-sequence model with parallel text corpus for language style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The article does not mention which layer works better on top of BERT. It only presents two extensions to the BERT model, RoBERT and ToBERT, which use either an LSTM network or another Transformer for classification. The article also notes that the Transformer's ability to capture long distance relationships between words in a sequence is an advantage over recurrent networks. Therefore, it is unclear which layer works better on top of BERT without further experimentation or analysis.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes, the authors believe that humans' robustness to noise is due to their ability to utilize general knowledge beyond what is contained in a given passage-question pair. They propose using inter-word semantic connections as a type of general knowledge to bridge the gap between MRC models and human beings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. They also use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They force the network to pay special attention to the middle context by repeating it. They also reduce the danger of the max pooling step choosing a value", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The dataset contains three major classes: Person (PER), Location (LOC), and Organization (ORG).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is higher quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The analyzed corpora are the largest French ones of their type available to date.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German dataset.", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The proposed model is compared to strong baselines models such as BIBREF18 and BIBREF20.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The article discusses the use of statistical machine learning models for event detection, specifically discriminative models such as deep neural networks. However, the article also mentions the use of logistic regression and multilayer perceptron as target models for comparison purposes.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, TensiStrength, CogComp-NLP, and spaCy.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The article mentions various approaches for modeling urban regions, identifying points-of-interest, and itineraries, as well as using Flickr for characterizing the natural environment. However, the usefulness of Flickr for characterizing the natural environment is less well-understood.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention in their model.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "The authors used three datasets for evaluation: CSAT, 20 newsgroups, and Fisher Phase 1 corpus.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, some of these tasks were evaluated in previous work. BIBREF1, BIBREF2, and BIBREF3 all evaluated different syntactic tasks. BIBREF5 also evaluated transformer-based models on the BIBREF1 agreement prediction dataset. BIBREF6 evaluated self-attention in the context of machine-translation and found that it performed on par with LSTM for syntax sensitive dependencies. However, the BERT model was evaluated in a bidirectional setting, which is different from previous work. The BERT results are not directly comparable to previous work due to differences in data, architecture, and evaluation methods. \n\nNote", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a constraint on the neural projector that requires the neural projection function to be invertible and to have a Jacobian determinant that is differentiable and nonzero. This condition enables tractable exact inference and marginal likelihood computation. The Jacobian regularization term accounts for the volume expansion or contraction behavior of the projection and prevents information loss during optimization. The invertibility condition is crucial during optimization, as it prevents the trivial solution of always projecting data to the same single point to maximize likelihood. The invertibility condition is satisfied by using an invertible transformation, such as the invertible transformation proposed by BIBREF16, which consists of a", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema looks like a taxonomy of the framework, with a full catalogue of features, their description, detailed annotation guideline, and illustrating examples in Appendix.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "WikiSmall dataset has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge dataset has 296,402 sentence pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines include Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train, and Cascaded systems.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Unanswerable.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings were used.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Their personalized models outperformed the baseline in BPE perplexity, and the Prior Name model achieved the best UMA and MRR by a large margin, revealing that prior recipe names are strong signals for personalization. Moreover, the addition of attention mechanisms to capture these signals improves language modeling performance over a strong non-personalized baseline.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the generated English poem may not work well with Shakespeare style transfer, as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to state-of-the-art results on existing, standard evaluation datasets.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution results showed that viral tweets containing fake news were created more recently, had a higher chance of coming from unverified accounts, had a larger ratio of friends/followers, and included more URLs to other sites than viral tweets without fake news. The content of viral fake news was also highly polarized. These findings suggest that it is possible to model and automatically detect fake news by understanding and modelling them.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The article does not mention any accents present in the corpus. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "A compact, scalable, and meaningful representation of the whole set of words.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11. B2. The second baseline assigns the value relevant to a pair INLINEFORM0, if and only if INLINEFORM1 appears in the title of INLINEFORM2.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Unanswerable. The article does not provide information about whether SemCor3.0 is representative of English language data in general. It only states that it is the largest corpus manually annotated with WordNet sense for WSD.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The size of the Augmented LibriSpeech dataset is 1,000 hours.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The authors used the high-quality datasets released by the challenge organizers for ternary and fine-grained sentiment classification. The fine-grained dataset was split into training, development, development_test, and test parts. The authors also used the pre-trained on tweets GloVe embeddings of BIBREF10. Additionally, they used 1,368 extra features consisting mostly of counts of punctuation symbols, emoticons, elongated words, and word membership features in several sentiment lexicons. These features were extracted using the open implementation of BIBREF2.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "The article discusses the potential for systematic biases in automatically constructed datasets, but does not address the issue of quality control. Therefore, the answer is \"unanswerable\".", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Unanswerable. The article does not mention anything about the domain of the images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive or even state-of-the-art for some of the emotion labels on standard evaluation datasets.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The article proposes a novel tagging scheme consisting of three tags: { INLINEFORM0 }, { INLINEFORM1 }, and { INLINEFORM2 }. The { INLINEFORM0 } tag indicates that the current word appears before the pun in the given context, the { INLINEFORM1 } tag highlights the current word as a pun, and the { INLINEFORM2 } tag indicates that the current word appears after the pun. The article also proposes a new tagging scheme consisting of three tags: { INLINEFORM0 }, { INLINEFORM1 }, and { INLINEFORM3 }, where { INLINEFORM0 } tag indicates that the current word appears before the pun in", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No, Arabic is not one of the 11 languages in CoVoST.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The article defines robustness of a model as the ability to handle situations where the prior knowledge supplied to the learning model is biased or incomplete. The authors investigate ways to reduce the sensitivity of the prior knowledge and make the model more robust and practical. They introduce auxiliary regularization terms in which their prior knowledge is formalized as a distribution over output variables. They also explore three regularization terms to address the problem: a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. The main contributions of this work are to reveal the factors of reducing the sensibility of the prior knowledge", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "InferSent and Universal Sentence Encoder are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets, and achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively for Chinese datasets. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against syntactic tree-based models, latent tree models, and non-tree models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection subtask.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the Neural Checklist Model and the Encoder-Decoder baseline with ingredient attention.", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The article suggests several methods to find examples of biases and unwarranted inferences, including manual detection, part-of-speech tagging, and leveraging the structure of Flickr30K Entities. The article also discusses the challenges of automatically detecting bias from the data and the importance of acknowledging stereotype-driven descriptions in language models.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "They explore French, Spanish, Italian, Portuguese, Hebrew, and Arabic.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Unanswerable. The article does not mention whether the results are reported only on English data or not.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with Sumy package algorithms and their own ILP-based approach.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was a probabilistic graphical model that used vocabulary-dependent latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize due to weak evaluation. The proposed models in this paper improve upon this state of the art by using neural models to predict instructor intervention in MOOC forums.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The least impactful component is the neighbors-only experiment, which showed a decrease in performance on all datasets.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: \"year [tab", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not explicitly stated in the article. However, the article mentions that the model achieves competitive performance compared with QANet trained on Chinese when fine-tuned on English but tested on Chinese. Additionally, the model shows transfer capability when training and testing sets are in different languages, especially between Chinese and Korean.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms baselines significantly.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article does not mention any specific improvement gained from Adversarial Reward Augmented Maximum Likelihood (ARAML). It only states that the model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the misclassifications of hate samples as offensive or neither, and by considering the content of tweets and the social context of tweeters. They also mention that previous studies have shown that many errors are due to biases from data collection and rules of annotation. Additionally, they note that the pre-trained BERT model has learned general knowledge from normal textual data without any purposely hateful or offensive language, which allows it to differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding. Overall, the authors suggest that their model can differentiate hate and offensive samples accurately by", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, several baselines were tested to compare with the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The size of the dataset is not specified in the article.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article does not mention any specific datasets used. However, it does mention using a neural network pretrained as a language model and fine-tuning it to directly predict ERP components. The authors also mention using eye-tracking, self-paced reading, and ERP components as heterogeneous types of data in the domain of human language processing prediction. Additionally, the authors plan to add studies using magnetoencephalography (MEG) activity, which is a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer’s Stone. Therefore, the answer is \"unanswerable\" for the specific datasets used in the article.</", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of visual stimuli, including a fixation cross, a house, and a face. The stimuli were presented in a randomized order and the subjects were instructed to press a button whenever they saw a face. The event-related responses were recorded using electroencephalography (EEG) and analyzed using a mixed neural network architecture. The dataset used in the study was publicly available and consisted of multimodal data for stimulus-based, imagined, and articulated speech states corresponding to 7 phonemic/syllabic categories and 4 words. The subjects were instructed to imagine speaking the words while the EEG data was recorded. The", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The article investigates the efficacy of different learning models in detecting abusive language, including traditional machine learning classifiers and neural network models such as Convolutional Neural Networks, Recurrent Neural Networks, and their variants.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "Bi-directional and uni-directional language models are used.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the confidence of the model's predictions.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies outperform the baseline A2C and KG-A2C, and both KG-A2C-chained and KG-A2C-Explore pass the bottleneck of a score of 40, while A2C-Explore gets to the bottleneck but cannot surpass it. KG-A2C-chained is significantly more sample efficient and converges faster, and chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. The knowledge graph cell representation appears to be a better indication of what a promising state is as opposed to just the textual observation.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual Bayesian model for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified through the use of diacritics such as apostrophes for sounds that are not found in Spanish. Additionally, the Mapuche team at the Instituto de Estudios Indígenas developed a supra-dialectal alphabet that comprises 28 letters that cover 32 phones used in the three Mapudungun variants. The main criterion for choosing alphabetic characters was to use the current Spanish keyboard that was available on all computers in Chilean offices and schools. The alphabet used the same letters used in Spanish for those phonemes that sound like Spanish phonemes. Diacritics such as apostrophes were", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network architecture that processes a sentence of words with misspelled characters, predicting the correct words at each step. It treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. Each word, represented accordingly, is then fed into a BiLSTM cell. At each sequence step, the training target is the correct corresponding word (output dimension equal to vocabulary size), and the model is optimized with cross-entropy loss.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The article explores 16 different languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. Additionally, the article mentions that the 16 languages considered in the experiments are typologically, morphologically, and syntactically fairly diverse. However, the article does not mention which specific languages are explored in the experiments. Therefore, the answer to this question is \"unanswerable\".", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms various baselines with a favorable generalization ability. It achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. For example, AIDA and xLisa perform quite well on ACE2004 but poorly on other datasets, or WAT, PBoH, and WNED have a favorable performance on CoNLL but lower values on ACE2004 and AQUAINT. Our proposed method performs consistently well on all datasets that demonstrates the good general", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes, the data is de-identified.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the CE task in 2010 i2b2/VA.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the model to generate more fluent and natural sequences by concentrating on one word at a time, based on the source document as well as other words. It also allows the model to utilize BERT's ability to generate high quality context vectors, which can enhance the decoder's performance. Additionally, masking words in the decoder can help to avoid phrase repetition by filtering out repeated tri-grams in beam-search process.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article does not mention the specific dataset used. It only discusses various models for learning unsupervised tweet representations.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. The dataset contains 9,473 annotations for 9,300 tweets. Each annotation is binarized as the positive class or negative class. The dataset is also encoded with 7 feature groups with associated feature values binarized to represent potentially informative features for classifying depression-related classes. The feature groups include lexical features, syntactic features, emotion features, demographic features, sentiment features, personality traits,", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "They evaluated on eight publicly available NER tasks used in BIBREF2.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated into Spanish using the machine translation platform Apertium. Additionally, the lexicons in the AffectiveTweets package were translated to Spanish, and the English version of SentiStrength was replaced by the Spanish variant made available by BIBREF6. The training set provided by BIBREF0 was also translated into Spanish to augment the training set.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the SLC task was a simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with baselines that do not adopt joint learning.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by assigning a political bias label to different US outlets and news articles following the procedure described in BIBREF2. In order to assess the robustness of the method, classification experiments are performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from 1.7K bilingual ancient-modern Chinese articles collected from the internet, with a large part of the ancient Chinese data coming from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The article does not specify the exact number of layers in the UTCNN model. However, it mentions that the model has a fully connected network and softmax is applied to yield the final stance label prediction for the post. Therefore, it can be inferred that the UTCNN model has at least one fully connected layer and one softmax layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the European network of nature protected sites Natura 2000 dataset. Additionally, the paper uses the same set of locations as in the species distribution experiments. The set of locations is defined as the 26,425 distinct sites occurring in the dataset.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of around 7,000 real medical reports written in Spanish and annotated with negation and uncertainty information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. The MEDDOCAN evaluation scenario is considered to be somewhat far from the real use case the technology developed for the shared task is supposed to be applied in. However, at the moment it also provides the only public means for a rigorous comparison between systems for sensitive health information detection in Spanish texts.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigrams and pragmatic features, stylistic patterns, patterns related to situational disparity, hashtag interpretations, emoticons, and laughter expressions.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are Coverage, Avg. MCC, and avg. +ve F1 score.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Unanswerable. The article does not mention anything about employing their indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The article describes experiments conducted on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. The article also presents case studies and error analyses of the experiments. Additionally, the article discusses the limitations of the current approach and proposes future directions for research.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. It also captures the representation of different directions to improve the ability of capturing the localness information and position information for CWS.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article does not specify which types of social media they considered. It only mentions Facebook and Twitter. However, it is likely that they considered other types of social media as well, given the general nature of the research question. Therefore, the answer is \"unanswerable\".", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the baseline CNN architecture. They are the features learned at hidden neuron in layer of the network. The baseline CNN extracts the inherent semantics from the sarcastic corpus by employing deep domain understanding. The process of using baseline features with other features extracted from the pre-trained model is described in Section SECREF24.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters and the type of word vectors were varied in the experiments on the four tasks.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Unanswerable. The article does not address this question.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to previous methods such as customized rule-based pattern matching, rule-based and machine learning approaches, CNN variants, and LSTM variants. However, the model achieves strong performance across multiple datasets using a single learned model, which is a departure from previous work that failed to generalize to medical domain questions. Additionally, the model is compared to state-of-the-art performance on TREC question classification, GARD corpus of consumer health questions, and MLBioMedLAT corpus of biomedical questions. The model also surpasses state-of-the-art performance on the GARD corpus and MLBioMedLAT dataset. Finally, the model is compared to previous work", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are larger compared to the previous ones.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 6946 sentences.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "Eusboost and MWMOTE techniques.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, their NER model learns NER from both text and images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a corpus of English text. However, they also mention that they use pre-trained word embeddings trained on the one billion word language modeling benchmark dataset, which includes text from multiple languages. Therefore, it is possible that they evaluate on datasets from other languages as well. However, this is not explicitly stated in the article.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "Their highest MRR score was 0.6103 in one of the test batches for Factoid Question Answering task.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The Wall Street Journal (WSJ) portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this claim by stating that they conducted a survey among engineers and identified a spectrum of three typical personas, and that the NLP toolkit they developed aims to satisfy the requirements of all three personas. They also mention that building models under general-purpose deep learning frameworks requires a large overhead of mastering these framework details, which hinders the productivity of engineers. Additionally, they note that many engineers find it difficult to choose from multiple frameworks, models, and optimization techniques, which results in less optimal solutions to their given tasks. Overall, the authors suggest that their toolkit can help engineers overcome these challenges and develop more efficient and effective DNN approaches for", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve the state of the art on both single-relation and multi-relation KBQA tasks. They also achieve a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP. They achieve a significant performance boost when using the top-3 relation detectors from Section \"Relation Detection Results\".", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
